# Import Library
# ì œì¶œ íŒŒì¼ ìƒì„± ê´€ë ¨
import os
import zipfile

# ë°ì´í„° ì²˜ë¦¬ ë° ë¶„ì„
import pandas as pd
import numpy as np
from scipy import stats
from tqdm import tqdm

# ë¨¸ì‹ ëŸ¬ë‹ ì „ì²˜ë¦¬
from sklearn.preprocessing import LabelEncoder, OrdinalEncoder

# ë¨¸ì‹ ëŸ¬ë‹ ëª¨ë¸
import xgboost as xgb

# í•©ì„± ë°ì´í„° ìƒì„±
from sdv.metadata import SingleTableMetadata
from sdv.single_table import CTGANSynthesizer

# To ignore all warnings
import warnings
warnings.filterwarnings('ignore')

# pycaret AutoML ì‚¬ìš©
import pycaret
from pycaret.classification import *
from pycaret.classification import ClassificationExperiment

print(pycaret.__version__)
# ìƒì„± ğŸ­
# Load Data
train_all = pd.read_csv("/workspace/Dataset/FSI/clip_downloads/FSI/train_features.csv")
test_all = pd.read_csv("/workspace/Dataset/FSI/clip_downloads/FSI/test_features.csv")
all_synthetic_data = pd.read_csv("/workspace/Dataset/FSI/clip_downloads/FSI/generated_data_features.csv")



## ì›ë³¸ ë°ì´í„°ì™€ concat
origin_train = train_all.drop(columns="ID")
all_synthetic_data = all_synthetic_data.drop(columns="ID")
train_total = pd.concat([origin_train, all_synthetic_data], ignore_index=True)
train_total.shape
train_total.head()



# í´ë” ìƒì„± ë° ì‘ì—… ë””ë ‰í† ë¦¬ ë³€ê²½
os.makedirs('/workspace/Dataset/FSI/clip_downloads/submission', exist_ok=True)
os.chdir("/workspace/Dataset/FSI/clip_downloads/submission")



# Data Preprocessing : pycaret setup / pycaret exp(init class)
exp = ClassificationExperiment()
print(type(exp))

exp.setup(train_total, target = 'Fraud_Type', session_id = 123) #setup data for pycaret experiment


# Compare Models : ëª¨ë¸ ì„ ì •(autoML)
print(exp.models())
best = exp.compare_models(include = ['xgboost']) #compare models by OOP method(nothing different from compare_models() function)


# save pipeline
exp.save_model(best, '/workspace/Dacon_FSI/baseline_modified_song/pycaret/best_model')


# Feature Imortance : Feature ì¤‘ìš”ë„ í™•ì¸(autoML)
# plot feature importance


# plot_model(best, plot = 'feature') #KNNë“±ì€ ì•ˆë ìˆ˜ë„ì‡ìŒ


# Prediction
# predict on test set
holdout_pred = exp.predict_model(best, data=test_all)
print(holdout_pred)


# Submission
# ë¶„ë¥˜ ì˜ˆì¸¡ ê²°ê³¼ ì œì¶œ ë°ì´í„°í”„ë ˆì„(DataFrame)
# ë¶„ë¥˜ ì˜ˆì¸¡ ê²°ê³¼ ë°ì´í„°í”„ë ˆì„ íŒŒì¼ëª…ì„ ë°˜ë“œì‹œ clf_submission.csv ë¡œ ì§€ì •í•´ì•¼í•©ë‹ˆë‹¤.
clf_submission = pd.read_csv("/workspace/Dataset/FSI/sample_submission.csv")
clf_submission["Fraud_Type"] = holdout_pred.prediction_label
clf_submission.head()



# í•©ì„± ë°ì´í„° ìƒì„± ê²°ê³¼ ì œì¶œ ë°ì´í„°í”„ë ˆì„(DataFrame)
# í•©ì„± ë°ì´í„° ìƒì„± ê²°ê³¼ ë°ì´í„°í”„ë ˆì„ íŒŒì¼ëª…ì„ ë°˜ë“œì‹œ syn_submission.csv ë¡œ ì§€ì •í•´ì•¼í•©ë‹ˆë‹¤.
all_synthetic_data.head()
'''
(*) ì €ì¥ ì‹œ ê° íŒŒì¼ëª…ì„ ë°˜ë“œì‹œ í™•ì¸í•´ì£¼ì„¸ìš”.
    1. ë¶„ë¥˜ ì˜ˆì¸¡ ê²°ê³¼ ë°ì´í„°í”„ë ˆì„ íŒŒì¼ëª… = clf_submission.csv
    2. í•©ì„± ë°ì´í„° ìƒì„± ê²°ê³¼ ë°ì´í„°í”„ë ˆì„ íŒŒì¼ëª… = syn_submission.csv

(*) ì œì¶œ íŒŒì¼(zip) ë‚´ì— ë‘ ê°œì˜ ë°ì´í„°í”„ë ˆì„ì´ ê°ê° ìœ„ì˜ íŒŒì¼ëª…ìœ¼ë¡œ ë°˜ë“œì‹œ ì¡´ì¬í•´ì•¼í•©ë‹ˆë‹¤.
(*) íŒŒì¼ëª…ì„ ì¼ì¹˜ì‹œí‚¤ì§€ ì•Šìœ¼ë©´ ì±„ì ì´ ë¶ˆê°€ëŠ¥í•©ë‹ˆë‹¤.
'''

# í´ë” ìƒì„± ë° ì‘ì—… ë””ë ‰í† ë¦¬ ë³€ê²½
os.makedirs('/workspace/Dataset/FSI/clip_downloads/submission', exist_ok=True)
os.chdir("/workspace/Dataset/FSI/clip_downloads/submission")

# CSV íŒŒì¼ë¡œ ì €ì¥
clf_submission.to_csv('/workspace/Dataset/FSI/clip_downloads/submissionclf_submission.csv', encoding='UTF-8-sig', index=False)
# all_synthetic_data.to_csv('/workspace/Dacon_FSI/baseline_modified_song/pycaret/submission/syn_submission.csv', encoding='UTF-8-sig', index=False)

# ZIP íŒŒì¼ ìƒì„± ë° CSV íŒŒì¼ ì¶”ê°€
with zipfile.ZipFile("/workspace/Dataset/FSI/clip_downloads/submission/baseline_submission.zip", 'w') as submission:
    submission.write('clf_submission.csv')
    submission.write('syn_submission.csv')
    
print('Done.')
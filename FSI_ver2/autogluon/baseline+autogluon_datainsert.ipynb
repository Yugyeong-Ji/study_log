{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Anomalous Financial Transaction Detection\n",
    "\n",
    "ë³¸ ëŒ€íšŒì˜ ê³¼ì œëŠ” ê¸ˆìœµ ê±°ë˜ ë°ì´í„°ì—ì„œ **ì´ìƒ ê±°ë˜ë¥¼ íƒì§€í•˜ëŠ” ê¸°ëŠ¥**ì„ ê°œì„ í•˜ê³  í™œìš©ë„ë¥¼ ë†’ì´ëŠ” ë¶„ë¥˜ AIëª¨ë¸ì„ ê°œë°œí•˜ëŠ” ê²ƒì…ë‹ˆë‹¤. \n",
    "\n",
    "íŠ¹íˆ, í´ë˜ìŠ¤ ë¶ˆê· í˜• ë¬¸ì œë¥¼ í•´ê²°í•˜ê¸° ìœ„í•´ ì˜¤í”ˆì†ŒìŠ¤ ìƒì„±í˜• AI ëª¨ë¸ì„ í™œìš©í•˜ì—¬ ë¶€ì¡±í•œ í´ë˜ìŠ¤ì˜ ë°ì´í„°ë¥¼ ë³´ì™„í•˜ê³ , ì´ë¥¼ í†µí•´ ë¶„ë¥˜ ëª¨ë¸ì˜ ì„±ëŠ¥ì„ í–¥ìƒì‹œí‚¤ëŠ” ê²ƒì´ í•µì‹¬ ëª©í‘œì…ë‹ˆë‹¤. \n",
    "\n",
    "ì´ëŸ¬í•œ ì ‘ê·¼ì„ í†µí•´ ê¸ˆìœµë³´ì•ˆì— íŠ¹í™”ëœ ë°ì´í„° ë¶„ì„ ë° í™œìš© ì—­ëŸ‰ì„ ê°•í™”í•˜ì—¬ ì „ë¬¸ ì¸ë ¥ì„ ì–‘ì„±í•˜ê³ , ê¸ˆìœµê¶Œì˜ AI í™œìš© ì–´ë ¤ì›€ì— ë”°ë¥¸ í•´ê²° ë°©ì•ˆì„ í•¨ê»˜ ëª¨ìƒ‰í•˜ë©° ê¸ˆìœµ ì‚°ì—…ì˜ AI í™œìš© í™œì„±í™”ë¥¼ ì§€ì›í•˜ëŠ” ê²ƒì„ ëª©í‘œë¡œ í•©ë‹ˆë‹¤."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import Library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ì œì¶œ íŒŒì¼ ìƒì„± ê´€ë ¨\n",
    "import os\n",
    "import zipfile\n",
    "\n",
    "# ë°ì´í„° ì²˜ë¦¬ ë° ë¶„ì„\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from scipy import stats\n",
    "from tqdm import tqdm\n",
    "\n",
    "# ë¨¸ì‹ ëŸ¬ë‹ ì „ì²˜ë¦¬\n",
    "from sklearn.preprocessing import LabelEncoder, OrdinalEncoder\n",
    "\n",
    "# ë¨¸ì‹ ëŸ¬ë‹ ëª¨ë¸\n",
    "import xgboost as xgb\n",
    "\n",
    "# í•©ì„± ë°ì´í„° ìƒì„±\n",
    "from sdv.metadata import SingleTableMetadata\n",
    "from sdv.single_table import CTGANSynthesizer\n",
    "\n",
    "# To ignore all warnings\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ìƒì„± ğŸ­"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_all = pd.read_csv(\"/workspace/Dataset/FSI/train.csv\")\n",
    "test_all = pd.read_csv(\"/workspace/Dataset/FSI/test.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = train_all.drop(columns=\"ID\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Fraud_Type\n",
       "m    118800\n",
       "a       100\n",
       "j       100\n",
       "h       100\n",
       "k       100\n",
       "c       100\n",
       "g       100\n",
       "i       100\n",
       "b       100\n",
       "f       100\n",
       "d       100\n",
       "e       100\n",
       "l       100\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train[\"Fraud_Type\"].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "(*) ë¦¬ë”ë³´ë“œ ì‚°ì‹ ì¤‘ ìƒì„±ë°ì´í„°ì˜ ìµëª…ì„±(TCAP)ì±„ì ì„ ìœ„í•´ ê° í´ë˜ìŠ¤ ë³„ë¡œ 1000ê°œì˜ ìƒì„±ë°ì´í„°ê°€ ë°˜ë“œì‹œ í•„ìš”í•©ë‹ˆë‹¤.\n",
    "(*) ë³¸ ë² ì´ìŠ¤ ë¼ì¸ì—ì„œëŠ” \"Fraud_Type\" 13ì¢…ë¥˜ì— ëŒ€í•´ 1000ê°œì”© , ì´ 13,000ê°œì˜ ë°ì´í„°ë¥¼ ìƒì„±í•  ì˜ˆì •ì…ë‹ˆë‹¤.\n",
    "(*) ë¶„ë¥˜ ëª¨ë¸ ì„±ëŠ¥ ê°œì„ ì„ ìœ„í•´ ìƒì„± ë°ì´í„°ë¥¼ í™œìš©í•˜ëŠ” ê²ƒì—ëŠ” ìƒì„± ë°ì´í„°ì˜ Row ê°œìˆ˜ì— ì œí•œì´ ì—†ìŠµë‹ˆë‹¤. ë‹¨, ë¦¬ë”ë³´ë“œ í‰ê°€ë¥¼ ìœ„í•´ ì œì¶œì„ í•˜ëŠ” ìƒì„± ë°ì´í„° í”„ë ˆì„ì€ ìµëª…ì„±(TCAP) í‰ê°€ë¥¼ ìœ„í•¨ì´ë©°, ìœ„ì˜ ì¡°ê±´ì„ ê°–ì¶˜ ìƒì„± ë°ì´í„°ë¥¼ ì œì¶œí•´ì•¼í•©ë‹ˆë‹¤.\n",
    "'''\n",
    "N_CLS_PER_GEN = 1000\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# # ì´ìƒì¹˜ ì²˜ë¦¬ í•¨ìˆ˜\n",
    "# def handle_outliers(series, n_std=3):\n",
    "#     mean = series.mean()\n",
    "#     std = series.std()\n",
    "#     z_scores = np.abs(stats.zscore(series))\n",
    "#     return series.mask(z_scores > n_std, mean)\n",
    "\n",
    "# # Time_difference ì»¬ëŸ¼ì„ ì´ ì´ˆë¡œ ë³€í™˜ ë° ì´ìƒì¹˜ ì²˜ë¦¬\n",
    "# train['Time_difference_seconds'] = pd.to_timedelta(train['Time_difference']).dt.total_seconds()\n",
    "# train['Time_difference_seconds'] = handle_outliers(train['Time_difference_seconds'])\n",
    "\n",
    "\n",
    "# # ëª¨ë“  Fraud_Type ëª©ë¡ ìƒì„± (m í¬í•¨)\n",
    "# fraud_types = train['Fraud_Type'].unique()\n",
    "\n",
    "# # ëª¨ë“  í•©ì„± ë°ì´í„°ë¥¼ ì €ì¥í•  DataFrame ì´ˆê¸°í™”\n",
    "# all_synthetic_data = pd.DataFrame()\n",
    "\n",
    "# N_SAMPLE = 100\n",
    "\n",
    "# # ê° Fraud_Typeì— ëŒ€í•´ í•©ì„± ë°ì´í„° ìƒì„± ë° ì €ì¥\n",
    "# for fraud_type in tqdm(fraud_types):\n",
    "    \n",
    "#     # í•´ë‹¹ Fraud_Typeì— ëŒ€í•œ ì„œë¸Œì…‹ ìƒì„±\n",
    "#     subset = train[train[\"Fraud_Type\"] == fraud_type]\n",
    "\n",
    "#     # ëª¨ë“  Fraud_Typeì— ëŒ€í•´ 100ê°œì”© ìƒ˜í”Œë§\n",
    "#     subset = subset.sample(n=N_SAMPLE, random_state=42)\n",
    "    \n",
    "#     # Time_difference ì—´ ì œì™¸ (ì´ˆ ë‹¨ìœ„ë¡œ ë³€í™˜ëœ ì»¬ëŸ¼ë§Œ ì‚¬ìš©)\n",
    "#     subset = subset.drop('Time_difference', axis=1)\n",
    "    \n",
    "#     # ë©”íƒ€ë°ì´í„° ìƒì„± ë° ëª¨ë¸ í•™ìŠµ\n",
    "#     metadata = SingleTableMetadata()\n",
    "\n",
    "#     metadata.detect_from_dataframe(subset)\n",
    "#     metadata.set_primary_key(None)\n",
    "\n",
    "#     # ë°ì´í„° íƒ€ì… ì„¤ì •\n",
    "#     column_sdtypes = {\n",
    "#         'Account_initial_balance': 'numerical',\n",
    "#         'Account_balance': 'numerical',\n",
    "#         'Customer_identification_number': 'categorical',  \n",
    "#         'Customer_personal_identifier': 'categorical',\n",
    "#         'Account_account_number': 'categorical',\n",
    "#         'IP_Address': 'ipv4_address',  \n",
    "#         'Location': 'categorical',\n",
    "#         'Recipient_Account_Number': 'categorical',\n",
    "#         'Fraud_Type': 'categorical',\n",
    "#         'Time_difference_seconds': 'numerical',\n",
    "#         'Customer_Birthyear': 'numerical'\n",
    "#     }\n",
    "\n",
    "#     # ê° ì»¬ëŸ¼ì— ëŒ€í•´ ë°ì´í„° íƒ€ì… ì„¤ì •\n",
    "#     for column, sdtype in column_sdtypes.items():\n",
    "#         metadata.update_column(\n",
    "#             column_name=column,\n",
    "#             sdtype=sdtype\n",
    "#         )\n",
    "        \n",
    "#     synthesizer = CTGANSynthesizer(\n",
    "#                             metadata,\n",
    "#                             epochs=100\n",
    "#                         )\n",
    "#     synthesizer.fit(subset)\n",
    "\n",
    "#     synthetic_subset = synthesizer.sample(num_rows=N_CLS_PER_GEN)\n",
    "    \n",
    "#     # ìƒì„±ëœ Time_difference_secondsì˜ ì´ìƒì¹˜ ì²˜ë¦¬\n",
    "#     synthetic_subset['Time_difference_seconds'] = handle_outliers(synthetic_subset['Time_difference_seconds'])\n",
    "    \n",
    "#     # Time_difference_secondsë¥¼ ë‹¤ì‹œ timedeltaë¡œ ë³€í™˜\n",
    "#     synthetic_subset['Time_difference'] = pd.to_timedelta(synthetic_subset['Time_difference_seconds'], unit='s')\n",
    "    \n",
    "#     # Time_difference_seconds ì»¬ëŸ¼ ì œê±°\n",
    "#     synthetic_subset = synthetic_subset.drop('Time_difference_seconds', axis=1)\n",
    "    \n",
    "#     # ìƒì„±ëœ ë°ì´í„°ë¥¼ all_synthetic_dataì— ì¶”ê°€\n",
    "#     all_synthetic_data = pd.concat([all_synthetic_data, synthetic_subset], ignore_index=True)\n",
    "# # ìµœì¢… ê²°ê³¼ í™•ì¸\n",
    "# print(\"\\nFinal All Synthetic Data Shape:\", all_synthetic_data.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# all_synthetic_data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ì›ë³¸ ë°ì´í„°ì™€ concat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(185000, 63)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "origin_train = train_all.drop(columns=\"ID\")\n",
    "all_synthetic_data = pd.read_csv(\"/workspace/Dacon_FSI/autogluon/filtered_data_3000.csv\")\n",
    "train_total = pd.concat([origin_train, all_synthetic_data])\n",
    "train_total.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Preprocessing 1 : Select x, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_x = train_total.drop(columns=['Fraud_Type'])\n",
    "train_y = train_total['Fraud_Type']\n",
    "\n",
    "test_x = test_all.drop(columns=['ID'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Preprocessing 2 : ë²”ì£¼í˜• ë³€ìˆ˜ ì¸ì½”ë”©"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ì›ë˜ ë ˆì´ë¸”: a, ë³€í™˜ëœ ìˆ«ì: 0\n",
      "ì›ë˜ ë ˆì´ë¸”: b, ë³€í™˜ëœ ìˆ«ì: 1\n",
      "ì›ë˜ ë ˆì´ë¸”: c, ë³€í™˜ëœ ìˆ«ì: 2\n",
      "ì›ë˜ ë ˆì´ë¸”: d, ë³€í™˜ëœ ìˆ«ì: 3\n",
      "ì›ë˜ ë ˆì´ë¸”: e, ë³€í™˜ëœ ìˆ«ì: 4\n",
      "ì›ë˜ ë ˆì´ë¸”: f, ë³€í™˜ëœ ìˆ«ì: 5\n",
      "ì›ë˜ ë ˆì´ë¸”: g, ë³€í™˜ëœ ìˆ«ì: 6\n",
      "ì›ë˜ ë ˆì´ë¸”: h, ë³€í™˜ëœ ìˆ«ì: 7\n",
      "ì›ë˜ ë ˆì´ë¸”: i, ë³€í™˜ëœ ìˆ«ì: 8\n",
      "ì›ë˜ ë ˆì´ë¸”: j, ë³€í™˜ëœ ìˆ«ì: 9\n",
      "ì›ë˜ ë ˆì´ë¸”: k, ë³€í™˜ëœ ìˆ«ì: 10\n",
      "ì›ë˜ ë ˆì´ë¸”: l, ë³€í™˜ëœ ìˆ«ì: 11\n",
      "ì›ë˜ ë ˆì´ë¸”: m, ë³€í™˜ëœ ìˆ«ì: 12\n"
     ]
    }
   ],
   "source": [
    "le_subclass = LabelEncoder()\n",
    "train_y_encoded = le_subclass.fit_transform(train_y)\n",
    "\n",
    "# ë³€í™˜ëœ ë ˆì´ë¸” í™•ì¸\n",
    "for i, label in enumerate(le_subclass.classes_):\n",
    "    print(f\"ì›ë˜ ë ˆì´ë¸”: {label}, ë³€í™˜ëœ ìˆ«ì: {i}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_x\n",
    "# 'Time_difference' ì—´ì„ ë¬¸ìì—´ë¡œ ë³€í™˜\n",
    "train_x['Time_difference'] = train_x['Time_difference'].astype(str)\n",
    "\n",
    "# ë²”ì£¼í˜• ë³€ìˆ˜ ì¸ì½”ë”©\n",
    "categorical_columns = train_x.select_dtypes(include=['object', 'category']).columns\n",
    "ordinal_encoder = OrdinalEncoder(handle_unknown='use_encoded_value', unknown_value=-1)\n",
    "\n",
    "# í›ˆë ¨ ë°ì´í„° ì¸ì½”ë”©\n",
    "train_x_encoded = train_x.copy()\n",
    "train_x_encoded[categorical_columns] = ordinal_encoder.fit_transform(train_x[categorical_columns])\n",
    "\n",
    "# íŠ¹ì„± ìˆœì„œ ì €ì¥\n",
    "feature_order = train_x_encoded.columns.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Define"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No path specified. Models will be saved in: \"AutogluonModels/ag-20240824_143209\"\n",
      "Verbosity: 2 (Standard Logging)\n",
      "=================== System Info ===================\n",
      "AutoGluon Version:  1.1.1\n",
      "Python Version:     3.10.13\n",
      "Operating System:   Linux\n",
      "Platform Machine:   x86_64\n",
      "Platform Version:   #1 SMP Fri Mar 29 23:14:13 UTC 2024\n",
      "CPU Count:          28\n",
      "Memory Avail:       23.92 GB / 31.23 GB (76.6%)\n",
      "Disk Space Avail:   895.26 GB / 1006.85 GB (88.9%)\n",
      "===================================================\n",
      "No presets specified! To achieve strong results with AutoGluon, it is recommended to use the available presets.\n",
      "\tRecommended Presets (For more details refer to https://auto.gluon.ai/stable/tutorials/tabular/tabular-essentials.html#presets):\n",
      "\tpresets='best_quality'   : Maximize accuracy. Default time_limit=3600.\n",
      "\tpresets='high_quality'   : Strong accuracy with fast inference speed. Default time_limit=3600.\n",
      "\tpresets='good_quality'   : Good accuracy with very fast inference speed. Default time_limit=3600.\n",
      "\tpresets='medium_quality' : Fast training time, ideal for initial prototyping.\n",
      "Warning: Training may take a very long time because `time_limit` was not specified and `train_data` is large (148000 samples, 75.78 MB).\n",
      "\tConsider setting `time_limit` to ensure training finishes within an expected duration or experiment with a small portion of `train_data` to identify an ideal `presets` and `hyperparameters` configuration.\n",
      "Beginning AutoGluon training ...\n",
      "AutoGluon will save models to \"AutogluonModels/ag-20240824_143209\"\n",
      "Train Data Rows:    148000\n",
      "Train Data Columns: 62\n",
      "Label Column:       Fraud_Type\n",
      "AutoGluon infers your prediction problem is: 'multiclass' (because dtype of label-column == int, but few unique label-values observed).\n",
      "\tFirst 10 (of 13) unique label values:  [10, 4, 0, 12, 2, 7, 8, 11, 1, 6]\n",
      "\tIf 'multiclass' is not the correct problem_type, please manually specify the problem_type parameter during Predictor init (You may specify problem_type as one of: ['binary', 'multiclass', 'regression', 'quantile'])\n",
      "Problem Type:       multiclass\n",
      "Preprocessing data ...\n",
      "Train Data Class Count: 13\n",
      "Using Feature Generators to preprocess the data ...\n",
      "Fitting AutoMLPipelineFeatureGenerator...\n",
      "\tAvailable Memory:                    24512.27 MB\n",
      "\tTrain Data (Original)  Memory Usage: 70.01 MB (0.3% of available memory)\n",
      "\tInferring data type of each feature based on column values. Set feature_metadata_in to manually specify special dtypes of the features.\n",
      "\tStage 1 Generators:\n",
      "\t\tFitting AsTypeFeatureGenerator...\n",
      "\t\t\tNote: Converting 27 features to boolean dtype as they only contain 2 unique values.\n",
      "\tStage 2 Generators:\n",
      "\t\tFitting FillNaFeatureGenerator...\n",
      "\tStage 3 Generators:\n",
      "\t\tFitting IdentityFeatureGenerator...\n",
      "\tStage 4 Generators:\n",
      "\t\tFitting DropUniqueFeatureGenerator...\n",
      "\tStage 5 Generators:\n",
      "\t\tFitting DropDuplicatesFeatureGenerator...\n",
      "\tUseless Original Features (Count: 1): ['Another_Person_Account']\n",
      "\t\tThese features carry no predictive signal and should be manually investigated.\n",
      "\t\tThis is typically a feature which has the same value for all rows.\n",
      "\t\tThese features do not need to be present at inference time.\n",
      "\tTypes of features in original data (raw dtype, special dtypes):\n",
      "\t\t('float', []) : 24 | ['Customer_Gender', 'Customer_personal_identifier', 'Customer_identification_number', 'Customer_registration_datetime', 'Customer_credit_rating', ...]\n",
      "\t\t('int', [])   : 37 | ['Customer_Birthyear', 'Customer_flag_change_of_authentication_1', 'Customer_flag_change_of_authentication_2', 'Customer_flag_change_of_authentication_3', 'Customer_flag_change_of_authentication_4', ...]\n",
      "\tTypes of features in processed data (raw dtype, special dtypes):\n",
      "\t\t('float', [])     : 21 | ['Customer_personal_identifier', 'Customer_identification_number', 'Customer_registration_datetime', 'Customer_credit_rating', 'Customer_loan_type', ...]\n",
      "\t\t('int', [])       : 13 | ['Customer_Birthyear', 'Account_initial_balance', 'Account_balance', 'Account_amount_daily_limit', 'Account_remaining_amount_daily_limit_exceeded', ...]\n",
      "\t\t('int', ['bool']) : 27 | ['Customer_Gender', 'Customer_flag_change_of_authentication_1', 'Customer_flag_change_of_authentication_2', 'Customer_flag_change_of_authentication_3', 'Customer_flag_change_of_authentication_4', ...]\n",
      "\t0.4s = Fit runtime\n",
      "\t61 features in original data used to generate 61 features in processed data.\n",
      "\tTrain Data (Processed) Memory Usage: 42.20 MB (0.2% of available memory)\n",
      "Data preprocessing and feature engineering runtime = 0.5s ...\n",
      "AutoGluon will gauge predictive performance using evaluation metric: 'accuracy'\n",
      "\tTo change this, specify the eval_metric parameter of Predictor()\n",
      "Automatically generating train/validation split with holdout_frac=0.016891891891891893, Train Rows: 145500, Val Rows: 2500\n",
      "User-specified model hyperparameters to be fit:\n",
      "{\n",
      "\t'NN_TORCH': {},\n",
      "\t'GBM': [{'extra_trees': True, 'ag_args': {'name_suffix': 'XT'}}, {}, 'GBMLarge'],\n",
      "\t'CAT': {},\n",
      "\t'XGB': {},\n",
      "\t'FASTAI': {},\n",
      "\t'RF': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n",
      "\t'XT': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n",
      "\t'KNN': [{'weights': 'uniform', 'ag_args': {'name_suffix': 'Unif'}}, {'weights': 'distance', 'ag_args': {'name_suffix': 'Dist'}}],\n",
      "}\n",
      "Fitting 13 L1 models ...\n",
      "Fitting model: KNeighborsUnif ...\n",
      "\t0.8804\t = Validation score   (accuracy)\n",
      "\t0.09s\t = Training   runtime\n",
      "\t0.17s\t = Validation runtime\n",
      "Fitting model: KNeighborsDist ...\n",
      "\t0.8804\t = Validation score   (accuracy)\n",
      "\t0.1s\t = Training   runtime\n",
      "\t0.15s\t = Validation runtime\n",
      "Fitting model: NeuralNetFastAI ...\n",
      "\t0.9916\t = Validation score   (accuracy)\n",
      "\t64.98s\t = Training   runtime\n",
      "\t0.02s\t = Validation runtime\n",
      "Fitting model: LightGBMXT ...\n",
      "\t0.9932\t = Validation score   (accuracy)\n",
      "\t8.55s\t = Training   runtime\n",
      "\t0.01s\t = Validation runtime\n",
      "Fitting model: LightGBM ...\n",
      "\t0.9952\t = Validation score   (accuracy)\n",
      "\t5.05s\t = Training   runtime\n",
      "\t0.01s\t = Validation runtime\n",
      "Fitting model: RandomForestGini ...\n",
      "\t0.9912\t = Validation score   (accuracy)\n",
      "\t11.89s\t = Training   runtime\n",
      "\t0.08s\t = Validation runtime\n",
      "Fitting model: RandomForestEntr ...\n",
      "\t0.9916\t = Validation score   (accuracy)\n",
      "\t11.67s\t = Training   runtime\n",
      "\t0.07s\t = Validation runtime\n",
      "Fitting model: CatBoost ...\n",
      "\t0.9932\t = Validation score   (accuracy)\n",
      "\t59.97s\t = Training   runtime\n",
      "\t0.0s\t = Validation runtime\n",
      "Fitting model: ExtraTreesGini ...\n",
      "\t0.9856\t = Validation score   (accuracy)\n",
      "\t4.07s\t = Training   runtime\n",
      "\t0.07s\t = Validation runtime\n",
      "Fitting model: ExtraTreesEntr ...\n",
      "\t0.9792\t = Validation score   (accuracy)\n",
      "\t3.93s\t = Training   runtime\n",
      "\t0.08s\t = Validation runtime\n",
      "Fitting model: XGBoost ...\n",
      "\t0.996\t = Validation score   (accuracy)\n",
      "\t7.45s\t = Training   runtime\n",
      "\t0.01s\t = Validation runtime\n",
      "Fitting model: NeuralNetTorch ...\n",
      "\t0.988\t = Validation score   (accuracy)\n",
      "\t32.65s\t = Training   runtime\n",
      "\t0.01s\t = Validation runtime\n",
      "Fitting model: LightGBMLarge ...\n",
      "\t0.994\t = Validation score   (accuracy)\n",
      "\t17.56s\t = Training   runtime\n",
      "\t0.03s\t = Validation runtime\n",
      "Fitting model: WeightedEnsemble_L2 ...\n",
      "\tEnsemble Weights: {'XGBoost': 1.0}\n",
      "\t0.996\t = Validation score   (accuracy)\n",
      "\t0.16s\t = Training   runtime\n",
      "\t0.0s\t = Validation runtime\n",
      "AutoGluon training complete, total runtime = 232.22s ... Best model: WeightedEnsemble_L2 | Estimated inference throughput: 167705.1 rows/s (2500 batch size)\n",
      "TabularPredictor saved. To load, use: predictor = TabularPredictor.load(\"AutogluonModels/ag-20240824_143209\")\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "y_true contains only one label (12). Please provide the true labels explicitly through the labels argument.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[12], line 35\u001b[0m\n\u001b[1;32m     33\u001b[0m \u001b[38;5;66;03m# Calculate accuracy and log loss for the current fraud type\u001b[39;00m\n\u001b[1;32m     34\u001b[0m accuracy \u001b[38;5;241m=\u001b[39m accuracy_score(true_labels, predictions)\n\u001b[0;32m---> 35\u001b[0m logloss \u001b[38;5;241m=\u001b[39m \u001b[43mlog_loss\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrue_labels\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mprobabilities\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     37\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFraud Type: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfraud_type\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     38\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m - Accuracy: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00maccuracy\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/sklearn/utils/_param_validation.py:213\u001b[0m, in \u001b[0;36mvalidate_params.<locals>.decorator.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    207\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    208\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m config_context(\n\u001b[1;32m    209\u001b[0m         skip_parameter_validation\u001b[38;5;241m=\u001b[39m(\n\u001b[1;32m    210\u001b[0m             prefer_skip_nested_validation \u001b[38;5;129;01mor\u001b[39;00m global_skip_validation\n\u001b[1;32m    211\u001b[0m         )\n\u001b[1;32m    212\u001b[0m     ):\n\u001b[0;32m--> 213\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    214\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m InvalidParameterError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    215\u001b[0m     \u001b[38;5;66;03m# When the function is just a wrapper around an estimator, we allow\u001b[39;00m\n\u001b[1;32m    216\u001b[0m     \u001b[38;5;66;03m# the function to delegate validation to the estimator, but we replace\u001b[39;00m\n\u001b[1;32m    217\u001b[0m     \u001b[38;5;66;03m# the name of the estimator by the name of the function in the error\u001b[39;00m\n\u001b[1;32m    218\u001b[0m     \u001b[38;5;66;03m# message to avoid confusion.\u001b[39;00m\n\u001b[1;32m    219\u001b[0m     msg \u001b[38;5;241m=\u001b[39m re\u001b[38;5;241m.\u001b[39msub(\n\u001b[1;32m    220\u001b[0m         \u001b[38;5;124mr\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mparameter of \u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mw+ must be\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m    221\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mparameter of \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfunc\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__qualname__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m must be\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m    222\u001b[0m         \u001b[38;5;28mstr\u001b[39m(e),\n\u001b[1;32m    223\u001b[0m     )\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/sklearn/metrics/_classification.py:2917\u001b[0m, in \u001b[0;36mlog_loss\u001b[0;34m(y_true, y_pred, eps, normalize, sample_weight, labels)\u001b[0m\n\u001b[1;32m   2915\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(lb\u001b[38;5;241m.\u001b[39mclasses_) \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[1;32m   2916\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m labels \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m-> 2917\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m   2918\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124my_true contains only one label (\u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;124m). Please \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   2919\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mprovide the true labels explicitly through the \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   2920\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlabels argument.\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(lb\u001b[38;5;241m.\u001b[39mclasses_[\u001b[38;5;241m0\u001b[39m])\n\u001b[1;32m   2921\u001b[0m         )\n\u001b[1;32m   2922\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   2923\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m   2924\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mThe labels array needs to contain at least two \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   2925\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlabels for log_loss, \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   2926\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mgot \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(lb\u001b[38;5;241m.\u001b[39mclasses_)\n\u001b[1;32m   2927\u001b[0m         )\n",
      "\u001b[0;31mValueError\u001b[0m: y_true contains only one label (12). Please provide the true labels explicitly through the labels argument."
     ]
    }
   ],
   "source": [
    "from autogluon.tabular import TabularPredictor\n",
    "\n",
    "\n",
    "train_data = train_x_encoded.copy()\n",
    "train_data['Fraud_Type'] = train_y_encoded\n",
    "\n",
    "from sklearn.metrics import accuracy_score, log_loss\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "train_data_split, val_data_split = train_test_split(train_data, test_size=0.2, random_state=42, stratify=train_data['Fraud_Type'])\n",
    "\n",
    "# Train the model\n",
    "predictor = TabularPredictor(label='Fraud_Type', eval_metric='accuracy').fit(train_data_split)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fraud Type: 12\n",
      " - Accuracy: 0.9997576736672051\n",
      " - Log Loss: Cannot calculate log loss with only one class present.\n",
      "\n",
      "Fraud Type: 9\n",
      " - Accuracy: 0.9852941176470589\n",
      " - Log Loss: Cannot calculate log loss with only one class present.\n",
      "\n",
      "Fraud Type: 7\n",
      " - Accuracy: 0.9872549019607844\n",
      " - Log Loss: Cannot calculate log loss with only one class present.\n",
      "\n",
      "Fraud Type: 2\n",
      " - Accuracy: 0.9705882352941176\n",
      " - Log Loss: Cannot calculate log loss with only one class present.\n",
      "\n",
      "Fraud Type: 11\n",
      " - Accuracy: 0.9745098039215686\n",
      " - Log Loss: Cannot calculate log loss with only one class present.\n",
      "\n",
      "Fraud Type: 10\n",
      " - Accuracy: 0.9911764705882353\n",
      " - Log Loss: Cannot calculate log loss with only one class present.\n",
      "\n",
      "Fraud Type: 8\n",
      " - Accuracy: 0.9774509803921568\n",
      " - Log Loss: Cannot calculate log loss with only one class present.\n",
      "\n",
      "Fraud Type: 4\n",
      " - Accuracy: 0.9901960784313726\n",
      " - Log Loss: Cannot calculate log loss with only one class present.\n",
      "\n",
      "Fraud Type: 5\n",
      " - Accuracy: 0.9911764705882353\n",
      " - Log Loss: Cannot calculate log loss with only one class present.\n",
      "\n",
      "Fraud Type: 0\n",
      " - Accuracy: 0.9941176470588236\n",
      " - Log Loss: Cannot calculate log loss with only one class present.\n",
      "\n",
      "Fraud Type: 6\n",
      " - Accuracy: 0.9862745098039216\n",
      " - Log Loss: Cannot calculate log loss with only one class present.\n",
      "\n",
      "Fraud Type: 1\n",
      " - Accuracy: 0.9980392156862745\n",
      " - Log Loss: Cannot calculate log loss with only one class present.\n",
      "\n",
      "Fraud Type: 3\n",
      " - Accuracy: 0.9754901960784313\n",
      " - Log Loss: Cannot calculate log loss with only one class present.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Make predictions on the validation set\n",
    "val_predictions = predictor.predict(val_data_split)\n",
    "val_probabilities = predictor.predict_proba(val_data_split)\n",
    "\n",
    "# Get true labels\n",
    "val_true_labels = val_data_split['Fraud_Type']\n",
    "\n",
    "# Calculate and print loss (accuracy and log loss) by fraud type\n",
    "fraud_types = val_true_labels.unique()\n",
    "for fraud_type in fraud_types:\n",
    "    # Get the indices for the current fraud type\n",
    "    indices = val_true_labels == fraud_type\n",
    "\n",
    "    # True labels and predictions for the current fraud type\n",
    "    true_labels = val_true_labels[indices]\n",
    "    predictions = val_predictions[indices]\n",
    "    probabilities = val_probabilities.loc[indices]\n",
    "\n",
    "    # Calculate accuracy\n",
    "    accuracy = accuracy_score(true_labels, predictions)\n",
    "    \n",
    "    # Handle single-class case for log loss\n",
    "    if len(true_labels.unique()) == 1:\n",
    "        print(f\"Fraud Type: {fraud_type}\")\n",
    "        print(f\" - Accuracy: {accuracy}\")\n",
    "        print(f\" - Log Loss: Cannot calculate log loss with only one class present.\\n\")\n",
    "    else:\n",
    "        logloss = log_loss(true_labels, probabilities)\n",
    "        print(f\"Fraud Type: {fraud_type}\")\n",
    "        print(f\" - Accuracy: {accuracy}\")\n",
    "        print(f\" - Log Loss: {logloss}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# í…ŒìŠ¤íŠ¸ ë°ì´í„° ì¸ì½”ë”©\n",
    "test_x_encoded = test_x.copy()\n",
    "test_x_encoded[categorical_columns] = ordinal_encoder.transform(test_x[categorical_columns])\n",
    "\n",
    "\n",
    "# íŠ¹ì„± ìˆœì„œ ë§ì¶”ê¸° ë° ë°ì´í„° íƒ€ì… ì¼ì¹˜\n",
    "test_x_encoded = test_x_encoded[feature_order]\n",
    "for col in feature_order:\n",
    "    test_x_encoded[col] = test_x_encoded[col].astype(train_x_encoded[col].dtype)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predict on test data\n",
    "predictions = predictor.predict(test_x_encoded)\n",
    "\n",
    "# Reverse transform to get original labels if necessary\n",
    "predictions_label = le_subclass.inverse_transform(predictions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Submission"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ID</th>\n",
       "      <th>Fraud_Type</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>TEST_000000</td>\n",
       "      <td>b</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>TEST_000001</td>\n",
       "      <td>m</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>TEST_000002</td>\n",
       "      <td>m</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>TEST_000003</td>\n",
       "      <td>m</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>TEST_000004</td>\n",
       "      <td>h</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            ID Fraud_Type\n",
       "0  TEST_000000          b\n",
       "1  TEST_000001          m\n",
       "2  TEST_000002          m\n",
       "3  TEST_000003          m\n",
       "4  TEST_000004          h"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# ë¶„ë¥˜ ì˜ˆì¸¡ ê²°ê³¼ ì œì¶œ ë°ì´í„°í”„ë ˆì„(DataFrame)\n",
    "# ë¶„ë¥˜ ì˜ˆì¸¡ ê²°ê³¼ ë°ì´í„°í”„ë ˆì„ íŒŒì¼ëª…ì„ ë°˜ë“œì‹œ clf_submission.csv ë¡œ ì§€ì •í•´ì•¼í•©ë‹ˆë‹¤.\n",
    "clf_submission = pd.read_csv(\"/workspace/Dataset/FSI/sample_submission.csv\")\n",
    "clf_submission[\"Fraud_Type\"] = predictions_label\n",
    "clf_submission.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Customer_Birthyear</th>\n",
       "      <th>Customer_Gender</th>\n",
       "      <th>Customer_personal_identifier</th>\n",
       "      <th>Customer_identification_number</th>\n",
       "      <th>Customer_registration_datetime</th>\n",
       "      <th>Customer_credit_rating</th>\n",
       "      <th>Customer_flag_change_of_authentication_1</th>\n",
       "      <th>Customer_flag_change_of_authentication_2</th>\n",
       "      <th>Customer_flag_change_of_authentication_3</th>\n",
       "      <th>Customer_flag_change_of_authentication_4</th>\n",
       "      <th>...</th>\n",
       "      <th>Last_atm_transaction_datetime</th>\n",
       "      <th>Last_bank_branch_transaction_datetime</th>\n",
       "      <th>Flag_deposit_more_than_tenMillion</th>\n",
       "      <th>Unused_account_status</th>\n",
       "      <th>Recipient_account_suspend_status</th>\n",
       "      <th>Number_of_transaction_with_the_account</th>\n",
       "      <th>Transaction_history_with_the_account</th>\n",
       "      <th>First_time_iOS_by_vulnerable_user</th>\n",
       "      <th>Fraud_Type</th>\n",
       "      <th>Transaction_resumed_date</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1977</td>\n",
       "      <td>male</td>\n",
       "      <td>ì¥ì˜ì‹</td>\n",
       "      <td>DuWOqP-DWQrklO</td>\n",
       "      <td>2012-12-10 22:02:43</td>\n",
       "      <td>B</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>2004-07-22 11:07:29</td>\n",
       "      <td>2012-04-30 20:27:48</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>m</td>\n",
       "      <td>2036-04-29 15:53:01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1973</td>\n",
       "      <td>male</td>\n",
       "      <td>ê°•ì§€ìš°</td>\n",
       "      <td>FZOPOt-CmkFKxG</td>\n",
       "      <td>2010-10-10 18:02:32</td>\n",
       "      <td>A</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>2013-07-09 21:00:28</td>\n",
       "      <td>2019-02-07 02:33:16</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>m</td>\n",
       "      <td>2011-12-18 17:32:43</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1979</td>\n",
       "      <td>male</td>\n",
       "      <td>ìš°ì§€í˜œ</td>\n",
       "      <td>LJfpJX-lNognsH</td>\n",
       "      <td>2012-12-10 22:02:43</td>\n",
       "      <td>B</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>2031-03-12 22:37:46</td>\n",
       "      <td>2011-09-10 13:02:51</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>m</td>\n",
       "      <td>2028-10-30 02:16:31</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2002</td>\n",
       "      <td>female</td>\n",
       "      <td>ì´ìœ¤ì„œ</td>\n",
       "      <td>KpdklD-ymHOSLQ</td>\n",
       "      <td>2007-06-08 19:44:42</td>\n",
       "      <td>B</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>2012-06-12 05:35:25</td>\n",
       "      <td>2025-06-14 07:48:55</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>m</td>\n",
       "      <td>2019-11-26 03:28:21</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1992</td>\n",
       "      <td>female</td>\n",
       "      <td>ìš°ì„œí˜„</td>\n",
       "      <td>BDBAtF-ZmBUHYl</td>\n",
       "      <td>2007-10-28 22:46:17</td>\n",
       "      <td>C</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>2009-03-24 13:53:00</td>\n",
       "      <td>2018-03-10 01:14:36</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>m</td>\n",
       "      <td>2025-06-30 21:01:03</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 63 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   Customer_Birthyear Customer_Gender Customer_personal_identifier  \\\n",
       "0                1977            male                          ì¥ì˜ì‹   \n",
       "1                1973            male                          ê°•ì§€ìš°   \n",
       "2                1979            male                          ìš°ì§€í˜œ   \n",
       "3                2002          female                          ì´ìœ¤ì„œ   \n",
       "4                1992          female                          ìš°ì„œí˜„   \n",
       "\n",
       "  Customer_identification_number Customer_registration_datetime  \\\n",
       "0                 DuWOqP-DWQrklO            2012-12-10 22:02:43   \n",
       "1                 FZOPOt-CmkFKxG            2010-10-10 18:02:32   \n",
       "2                 LJfpJX-lNognsH            2012-12-10 22:02:43   \n",
       "3                 KpdklD-ymHOSLQ            2007-06-08 19:44:42   \n",
       "4                 BDBAtF-ZmBUHYl            2007-10-28 22:46:17   \n",
       "\n",
       "  Customer_credit_rating  Customer_flag_change_of_authentication_1  \\\n",
       "0                      B                                         1   \n",
       "1                      A                                         1   \n",
       "2                      B                                         1   \n",
       "3                      B                                         1   \n",
       "4                      C                                         1   \n",
       "\n",
       "   Customer_flag_change_of_authentication_2  \\\n",
       "0                                         1   \n",
       "1                                         1   \n",
       "2                                         1   \n",
       "3                                         1   \n",
       "4                                         1   \n",
       "\n",
       "   Customer_flag_change_of_authentication_3  \\\n",
       "0                                         1   \n",
       "1                                         1   \n",
       "2                                         0   \n",
       "3                                         1   \n",
       "4                                         1   \n",
       "\n",
       "   Customer_flag_change_of_authentication_4  ...  \\\n",
       "0                                         1  ...   \n",
       "1                                         1  ...   \n",
       "2                                         1  ...   \n",
       "3                                         1  ...   \n",
       "4                                         1  ...   \n",
       "\n",
       "   Last_atm_transaction_datetime  Last_bank_branch_transaction_datetime  \\\n",
       "0            2004-07-22 11:07:29                    2012-04-30 20:27:48   \n",
       "1            2013-07-09 21:00:28                    2019-02-07 02:33:16   \n",
       "2            2031-03-12 22:37:46                    2011-09-10 13:02:51   \n",
       "3            2012-06-12 05:35:25                    2025-06-14 07:48:55   \n",
       "4            2009-03-24 13:53:00                    2018-03-10 01:14:36   \n",
       "\n",
       "   Flag_deposit_more_than_tenMillion Unused_account_status  \\\n",
       "0                                  0                     0   \n",
       "1                                  1                     1   \n",
       "2                                  0                     1   \n",
       "3                                  0                     1   \n",
       "4                                  0                     1   \n",
       "\n",
       "   Recipient_account_suspend_status  Number_of_transaction_with_the_account  \\\n",
       "0                                 1                                       0   \n",
       "1                                 1                                       0   \n",
       "2                                 0                                       0   \n",
       "3                                 0                                       0   \n",
       "4                                 0                                       2   \n",
       "\n",
       "   Transaction_history_with_the_account  First_time_iOS_by_vulnerable_user  \\\n",
       "0                                     0                                  0   \n",
       "1                                     2                                  0   \n",
       "2                                     0                                  0   \n",
       "3                                     2                                  0   \n",
       "4                                     0                                  0   \n",
       "\n",
       "   Fraud_Type  Transaction_resumed_date  \n",
       "0           m       2036-04-29 15:53:01  \n",
       "1           m       2011-12-18 17:32:43  \n",
       "2           m       2028-10-30 02:16:31  \n",
       "3           m       2019-11-26 03:28:21  \n",
       "4           m       2025-06-30 21:01:03  \n",
       "\n",
       "[5 rows x 63 columns]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# í•©ì„± ë°ì´í„° ìƒì„± ê²°ê³¼ ì œì¶œ ë°ì´í„°í”„ë ˆì„(DataFrame)\n",
    "# í•©ì„± ë°ì´í„° ìƒì„± ê²°ê³¼ ë°ì´í„°í”„ë ˆì„ íŒŒì¼ëª…ì„ ë°˜ë“œì‹œ syn_submission.csv ë¡œ ì§€ì •í•´ì•¼í•©ë‹ˆë‹¤.\n",
    "all_synthetic_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done.\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "(*) ì €ì¥ ì‹œ ê° íŒŒì¼ëª…ì„ ë°˜ë“œì‹œ í™•ì¸í•´ì£¼ì„¸ìš”.\n",
    "    1. ë¶„ë¥˜ ì˜ˆì¸¡ ê²°ê³¼ ë°ì´í„°í”„ë ˆì„ íŒŒì¼ëª… = clf_submission.csv\n",
    "    2. í•©ì„± ë°ì´í„° ìƒì„± ê²°ê³¼ ë°ì´í„°í”„ë ˆì„ íŒŒì¼ëª… = syn_submission.csv\n",
    "\n",
    "(*) ì œì¶œ íŒŒì¼(zip) ë‚´ì— ë‘ ê°œì˜ ë°ì´í„°í”„ë ˆì„ì´ ê°ê° ìœ„ì˜ íŒŒì¼ëª…ìœ¼ë¡œ ë°˜ë“œì‹œ ì¡´ì¬í•´ì•¼í•©ë‹ˆë‹¤.\n",
    "(*) íŒŒì¼ëª…ì„ ì¼ì¹˜ì‹œí‚¤ì§€ ì•Šìœ¼ë©´ ì±„ì ì´ ë¶ˆê°€ëŠ¥í•©ë‹ˆë‹¤.\n",
    "'''\n",
    "\n",
    "# í´ë” ìƒì„± ë° ì‘ì—… ë””ë ‰í† ë¦¬ ë³€ê²½\n",
    "os.makedirs('/workspace/Dataset/FSI/baseline+autogluon+datainsert/submission', exist_ok=True)\n",
    "os.chdir(\"/workspace/Dataset/FSI/baseline+autogluon+datainsert/submission/\")\n",
    "\n",
    "# CSV íŒŒì¼ë¡œ ì €ì¥\n",
    "clf_submission.to_csv('/workspace/Dataset/FSI/baseline+autogluon+datainsert/submission/clf_submission.csv', encoding='UTF-8-sig', index=False)\n",
    "all_synthetic_data.to_csv('/workspace/Dataset/FSI/baseline+autogluon+datainsert/submission/syn_submission.csv', encoding='UTF-8-sig', index=False)\n",
    "\n",
    "# ZIP íŒŒì¼ ìƒì„± ë° CSV íŒŒì¼ ì¶”ê°€\n",
    "with zipfile.ZipFile(\"/workspace/Dataset/FSI/baseline+autogluon+datainsert/submission/baseline_submission.zip\", 'w') as submission:\n",
    "    submission.write('clf_submission.csv')\n",
    "    submission.write('syn_submission.csv')\n",
    "    \n",
    "print('Done.')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

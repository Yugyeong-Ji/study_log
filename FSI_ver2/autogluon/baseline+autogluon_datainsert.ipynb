{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Anomalous Financial Transaction Detection\n",
    "\n",
    "본 대회의 과제는 금융 거래 데이터에서 **이상 거래를 탐지하는 기능**을 개선하고 활용도를 높이는 분류 AI모델을 개발하는 것입니다. \n",
    "\n",
    "특히, 클래스 불균형 문제를 해결하기 위해 오픈소스 생성형 AI 모델을 활용하여 부족한 클래스의 데이터를 보완하고, 이를 통해 분류 모델의 성능을 향상시키는 것이 핵심 목표입니다. \n",
    "\n",
    "이러한 접근을 통해 금융보안에 특화된 데이터 분석 및 활용 역량을 강화하여 전문 인력을 양성하고, 금융권의 AI 활용 어려움에 따른 해결 방안을 함께 모색하며 금융 산업의 AI 활용 활성화를 지원하는 것을 목표로 합니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import Library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 제출 파일 생성 관련\n",
    "import os\n",
    "import zipfile\n",
    "\n",
    "# 데이터 처리 및 분석\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from scipy import stats\n",
    "from tqdm import tqdm\n",
    "\n",
    "# 머신러닝 전처리\n",
    "from sklearn.preprocessing import LabelEncoder, OrdinalEncoder\n",
    "\n",
    "# 머신러닝 모델\n",
    "import xgboost as xgb\n",
    "\n",
    "# 합성 데이터 생성\n",
    "from sdv.metadata import SingleTableMetadata\n",
    "from sdv.single_table import CTGANSynthesizer\n",
    "\n",
    "# To ignore all warnings\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 생성 🏭"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_all = pd.read_csv(\"/workspace/Dataset/FSI/train.csv\")\n",
    "test_all = pd.read_csv(\"/workspace/Dataset/FSI/test.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = train_all.drop(columns=\"ID\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Fraud_Type\n",
       "m    118800\n",
       "a       100\n",
       "j       100\n",
       "h       100\n",
       "k       100\n",
       "c       100\n",
       "g       100\n",
       "i       100\n",
       "b       100\n",
       "f       100\n",
       "d       100\n",
       "e       100\n",
       "l       100\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train[\"Fraud_Type\"].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "(*) 리더보드 산식 중 생성데이터의 익명성(TCAP)채점을 위해 각 클래스 별로 1000개의 생성데이터가 반드시 필요합니다.\n",
    "(*) 본 베이스 라인에서는 \"Fraud_Type\" 13종류에 대해 1000개씩 , 총 13,000개의 데이터를 생성할 예정입니다.\n",
    "(*) 분류 모델 성능 개선을 위해 생성 데이터를 활용하는 것에는 생성 데이터의 Row 개수에 제한이 없습니다. 단, 리더보드 평가를 위해 제출을 하는 생성 데이터 프레임은 익명성(TCAP) 평가를 위함이며, 위의 조건을 갖춘 생성 데이터를 제출해야합니다.\n",
    "'''\n",
    "N_CLS_PER_GEN = 1000\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# # 이상치 처리 함수\n",
    "# def handle_outliers(series, n_std=3):\n",
    "#     mean = series.mean()\n",
    "#     std = series.std()\n",
    "#     z_scores = np.abs(stats.zscore(series))\n",
    "#     return series.mask(z_scores > n_std, mean)\n",
    "\n",
    "# # Time_difference 컬럼을 총 초로 변환 및 이상치 처리\n",
    "# train['Time_difference_seconds'] = pd.to_timedelta(train['Time_difference']).dt.total_seconds()\n",
    "# train['Time_difference_seconds'] = handle_outliers(train['Time_difference_seconds'])\n",
    "\n",
    "\n",
    "# # 모든 Fraud_Type 목록 생성 (m 포함)\n",
    "# fraud_types = train['Fraud_Type'].unique()\n",
    "\n",
    "# # 모든 합성 데이터를 저장할 DataFrame 초기화\n",
    "# all_synthetic_data = pd.DataFrame()\n",
    "\n",
    "# N_SAMPLE = 100\n",
    "\n",
    "# # 각 Fraud_Type에 대해 합성 데이터 생성 및 저장\n",
    "# for fraud_type in tqdm(fraud_types):\n",
    "    \n",
    "#     # 해당 Fraud_Type에 대한 서브셋 생성\n",
    "#     subset = train[train[\"Fraud_Type\"] == fraud_type]\n",
    "\n",
    "#     # 모든 Fraud_Type에 대해 100개씩 샘플링\n",
    "#     subset = subset.sample(n=N_SAMPLE, random_state=42)\n",
    "    \n",
    "#     # Time_difference 열 제외 (초 단위로 변환된 컬럼만 사용)\n",
    "#     subset = subset.drop('Time_difference', axis=1)\n",
    "    \n",
    "#     # 메타데이터 생성 및 모델 학습\n",
    "#     metadata = SingleTableMetadata()\n",
    "\n",
    "#     metadata.detect_from_dataframe(subset)\n",
    "#     metadata.set_primary_key(None)\n",
    "\n",
    "#     # 데이터 타입 설정\n",
    "#     column_sdtypes = {\n",
    "#         'Account_initial_balance': 'numerical',\n",
    "#         'Account_balance': 'numerical',\n",
    "#         'Customer_identification_number': 'categorical',  \n",
    "#         'Customer_personal_identifier': 'categorical',\n",
    "#         'Account_account_number': 'categorical',\n",
    "#         'IP_Address': 'ipv4_address',  \n",
    "#         'Location': 'categorical',\n",
    "#         'Recipient_Account_Number': 'categorical',\n",
    "#         'Fraud_Type': 'categorical',\n",
    "#         'Time_difference_seconds': 'numerical',\n",
    "#         'Customer_Birthyear': 'numerical'\n",
    "#     }\n",
    "\n",
    "#     # 각 컬럼에 대해 데이터 타입 설정\n",
    "#     for column, sdtype in column_sdtypes.items():\n",
    "#         metadata.update_column(\n",
    "#             column_name=column,\n",
    "#             sdtype=sdtype\n",
    "#         )\n",
    "        \n",
    "#     synthesizer = CTGANSynthesizer(\n",
    "#                             metadata,\n",
    "#                             epochs=100\n",
    "#                         )\n",
    "#     synthesizer.fit(subset)\n",
    "\n",
    "#     synthetic_subset = synthesizer.sample(num_rows=N_CLS_PER_GEN)\n",
    "    \n",
    "#     # 생성된 Time_difference_seconds의 이상치 처리\n",
    "#     synthetic_subset['Time_difference_seconds'] = handle_outliers(synthetic_subset['Time_difference_seconds'])\n",
    "    \n",
    "#     # Time_difference_seconds를 다시 timedelta로 변환\n",
    "#     synthetic_subset['Time_difference'] = pd.to_timedelta(synthetic_subset['Time_difference_seconds'], unit='s')\n",
    "    \n",
    "#     # Time_difference_seconds 컬럼 제거\n",
    "#     synthetic_subset = synthetic_subset.drop('Time_difference_seconds', axis=1)\n",
    "    \n",
    "#     # 생성된 데이터를 all_synthetic_data에 추가\n",
    "#     all_synthetic_data = pd.concat([all_synthetic_data, synthetic_subset], ignore_index=True)\n",
    "# # 최종 결과 확인\n",
    "# print(\"\\nFinal All Synthetic Data Shape:\", all_synthetic_data.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# all_synthetic_data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 원본 데이터와 concat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(185000, 63)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "origin_train = train_all.drop(columns=\"ID\")\n",
    "all_synthetic_data = pd.read_csv(\"/workspace/Dacon_FSI/autogluon/filtered_data_3000.csv\")\n",
    "train_total = pd.concat([origin_train, all_synthetic_data])\n",
    "train_total.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Preprocessing 1 : Select x, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_x = train_total.drop(columns=['Fraud_Type'])\n",
    "train_y = train_total['Fraud_Type']\n",
    "\n",
    "test_x = test_all.drop(columns=['ID'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Preprocessing 2 : 범주형 변수 인코딩"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "원래 레이블: a, 변환된 숫자: 0\n",
      "원래 레이블: b, 변환된 숫자: 1\n",
      "원래 레이블: c, 변환된 숫자: 2\n",
      "원래 레이블: d, 변환된 숫자: 3\n",
      "원래 레이블: e, 변환된 숫자: 4\n",
      "원래 레이블: f, 변환된 숫자: 5\n",
      "원래 레이블: g, 변환된 숫자: 6\n",
      "원래 레이블: h, 변환된 숫자: 7\n",
      "원래 레이블: i, 변환된 숫자: 8\n",
      "원래 레이블: j, 변환된 숫자: 9\n",
      "원래 레이블: k, 변환된 숫자: 10\n",
      "원래 레이블: l, 변환된 숫자: 11\n",
      "원래 레이블: m, 변환된 숫자: 12\n"
     ]
    }
   ],
   "source": [
    "le_subclass = LabelEncoder()\n",
    "train_y_encoded = le_subclass.fit_transform(train_y)\n",
    "\n",
    "# 변환된 레이블 확인\n",
    "for i, label in enumerate(le_subclass.classes_):\n",
    "    print(f\"원래 레이블: {label}, 변환된 숫자: {i}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_x\n",
    "# 'Time_difference' 열을 문자열로 변환\n",
    "train_x['Time_difference'] = train_x['Time_difference'].astype(str)\n",
    "\n",
    "# 범주형 변수 인코딩\n",
    "categorical_columns = train_x.select_dtypes(include=['object', 'category']).columns\n",
    "ordinal_encoder = OrdinalEncoder(handle_unknown='use_encoded_value', unknown_value=-1)\n",
    "\n",
    "# 훈련 데이터 인코딩\n",
    "train_x_encoded = train_x.copy()\n",
    "train_x_encoded[categorical_columns] = ordinal_encoder.fit_transform(train_x[categorical_columns])\n",
    "\n",
    "# 특성 순서 저장\n",
    "feature_order = train_x_encoded.columns.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Define"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No path specified. Models will be saved in: \"AutogluonModels/ag-20240824_143209\"\n",
      "Verbosity: 2 (Standard Logging)\n",
      "=================== System Info ===================\n",
      "AutoGluon Version:  1.1.1\n",
      "Python Version:     3.10.13\n",
      "Operating System:   Linux\n",
      "Platform Machine:   x86_64\n",
      "Platform Version:   #1 SMP Fri Mar 29 23:14:13 UTC 2024\n",
      "CPU Count:          28\n",
      "Memory Avail:       23.92 GB / 31.23 GB (76.6%)\n",
      "Disk Space Avail:   895.26 GB / 1006.85 GB (88.9%)\n",
      "===================================================\n",
      "No presets specified! To achieve strong results with AutoGluon, it is recommended to use the available presets.\n",
      "\tRecommended Presets (For more details refer to https://auto.gluon.ai/stable/tutorials/tabular/tabular-essentials.html#presets):\n",
      "\tpresets='best_quality'   : Maximize accuracy. Default time_limit=3600.\n",
      "\tpresets='high_quality'   : Strong accuracy with fast inference speed. Default time_limit=3600.\n",
      "\tpresets='good_quality'   : Good accuracy with very fast inference speed. Default time_limit=3600.\n",
      "\tpresets='medium_quality' : Fast training time, ideal for initial prototyping.\n",
      "Warning: Training may take a very long time because `time_limit` was not specified and `train_data` is large (148000 samples, 75.78 MB).\n",
      "\tConsider setting `time_limit` to ensure training finishes within an expected duration or experiment with a small portion of `train_data` to identify an ideal `presets` and `hyperparameters` configuration.\n",
      "Beginning AutoGluon training ...\n",
      "AutoGluon will save models to \"AutogluonModels/ag-20240824_143209\"\n",
      "Train Data Rows:    148000\n",
      "Train Data Columns: 62\n",
      "Label Column:       Fraud_Type\n",
      "AutoGluon infers your prediction problem is: 'multiclass' (because dtype of label-column == int, but few unique label-values observed).\n",
      "\tFirst 10 (of 13) unique label values:  [10, 4, 0, 12, 2, 7, 8, 11, 1, 6]\n",
      "\tIf 'multiclass' is not the correct problem_type, please manually specify the problem_type parameter during Predictor init (You may specify problem_type as one of: ['binary', 'multiclass', 'regression', 'quantile'])\n",
      "Problem Type:       multiclass\n",
      "Preprocessing data ...\n",
      "Train Data Class Count: 13\n",
      "Using Feature Generators to preprocess the data ...\n",
      "Fitting AutoMLPipelineFeatureGenerator...\n",
      "\tAvailable Memory:                    24512.27 MB\n",
      "\tTrain Data (Original)  Memory Usage: 70.01 MB (0.3% of available memory)\n",
      "\tInferring data type of each feature based on column values. Set feature_metadata_in to manually specify special dtypes of the features.\n",
      "\tStage 1 Generators:\n",
      "\t\tFitting AsTypeFeatureGenerator...\n",
      "\t\t\tNote: Converting 27 features to boolean dtype as they only contain 2 unique values.\n",
      "\tStage 2 Generators:\n",
      "\t\tFitting FillNaFeatureGenerator...\n",
      "\tStage 3 Generators:\n",
      "\t\tFitting IdentityFeatureGenerator...\n",
      "\tStage 4 Generators:\n",
      "\t\tFitting DropUniqueFeatureGenerator...\n",
      "\tStage 5 Generators:\n",
      "\t\tFitting DropDuplicatesFeatureGenerator...\n",
      "\tUseless Original Features (Count: 1): ['Another_Person_Account']\n",
      "\t\tThese features carry no predictive signal and should be manually investigated.\n",
      "\t\tThis is typically a feature which has the same value for all rows.\n",
      "\t\tThese features do not need to be present at inference time.\n",
      "\tTypes of features in original data (raw dtype, special dtypes):\n",
      "\t\t('float', []) : 24 | ['Customer_Gender', 'Customer_personal_identifier', 'Customer_identification_number', 'Customer_registration_datetime', 'Customer_credit_rating', ...]\n",
      "\t\t('int', [])   : 37 | ['Customer_Birthyear', 'Customer_flag_change_of_authentication_1', 'Customer_flag_change_of_authentication_2', 'Customer_flag_change_of_authentication_3', 'Customer_flag_change_of_authentication_4', ...]\n",
      "\tTypes of features in processed data (raw dtype, special dtypes):\n",
      "\t\t('float', [])     : 21 | ['Customer_personal_identifier', 'Customer_identification_number', 'Customer_registration_datetime', 'Customer_credit_rating', 'Customer_loan_type', ...]\n",
      "\t\t('int', [])       : 13 | ['Customer_Birthyear', 'Account_initial_balance', 'Account_balance', 'Account_amount_daily_limit', 'Account_remaining_amount_daily_limit_exceeded', ...]\n",
      "\t\t('int', ['bool']) : 27 | ['Customer_Gender', 'Customer_flag_change_of_authentication_1', 'Customer_flag_change_of_authentication_2', 'Customer_flag_change_of_authentication_3', 'Customer_flag_change_of_authentication_4', ...]\n",
      "\t0.4s = Fit runtime\n",
      "\t61 features in original data used to generate 61 features in processed data.\n",
      "\tTrain Data (Processed) Memory Usage: 42.20 MB (0.2% of available memory)\n",
      "Data preprocessing and feature engineering runtime = 0.5s ...\n",
      "AutoGluon will gauge predictive performance using evaluation metric: 'accuracy'\n",
      "\tTo change this, specify the eval_metric parameter of Predictor()\n",
      "Automatically generating train/validation split with holdout_frac=0.016891891891891893, Train Rows: 145500, Val Rows: 2500\n",
      "User-specified model hyperparameters to be fit:\n",
      "{\n",
      "\t'NN_TORCH': {},\n",
      "\t'GBM': [{'extra_trees': True, 'ag_args': {'name_suffix': 'XT'}}, {}, 'GBMLarge'],\n",
      "\t'CAT': {},\n",
      "\t'XGB': {},\n",
      "\t'FASTAI': {},\n",
      "\t'RF': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n",
      "\t'XT': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n",
      "\t'KNN': [{'weights': 'uniform', 'ag_args': {'name_suffix': 'Unif'}}, {'weights': 'distance', 'ag_args': {'name_suffix': 'Dist'}}],\n",
      "}\n",
      "Fitting 13 L1 models ...\n",
      "Fitting model: KNeighborsUnif ...\n",
      "\t0.8804\t = Validation score   (accuracy)\n",
      "\t0.09s\t = Training   runtime\n",
      "\t0.17s\t = Validation runtime\n",
      "Fitting model: KNeighborsDist ...\n",
      "\t0.8804\t = Validation score   (accuracy)\n",
      "\t0.1s\t = Training   runtime\n",
      "\t0.15s\t = Validation runtime\n",
      "Fitting model: NeuralNetFastAI ...\n",
      "\t0.9916\t = Validation score   (accuracy)\n",
      "\t64.98s\t = Training   runtime\n",
      "\t0.02s\t = Validation runtime\n",
      "Fitting model: LightGBMXT ...\n",
      "\t0.9932\t = Validation score   (accuracy)\n",
      "\t8.55s\t = Training   runtime\n",
      "\t0.01s\t = Validation runtime\n",
      "Fitting model: LightGBM ...\n",
      "\t0.9952\t = Validation score   (accuracy)\n",
      "\t5.05s\t = Training   runtime\n",
      "\t0.01s\t = Validation runtime\n",
      "Fitting model: RandomForestGini ...\n",
      "\t0.9912\t = Validation score   (accuracy)\n",
      "\t11.89s\t = Training   runtime\n",
      "\t0.08s\t = Validation runtime\n",
      "Fitting model: RandomForestEntr ...\n",
      "\t0.9916\t = Validation score   (accuracy)\n",
      "\t11.67s\t = Training   runtime\n",
      "\t0.07s\t = Validation runtime\n",
      "Fitting model: CatBoost ...\n",
      "\t0.9932\t = Validation score   (accuracy)\n",
      "\t59.97s\t = Training   runtime\n",
      "\t0.0s\t = Validation runtime\n",
      "Fitting model: ExtraTreesGini ...\n",
      "\t0.9856\t = Validation score   (accuracy)\n",
      "\t4.07s\t = Training   runtime\n",
      "\t0.07s\t = Validation runtime\n",
      "Fitting model: ExtraTreesEntr ...\n",
      "\t0.9792\t = Validation score   (accuracy)\n",
      "\t3.93s\t = Training   runtime\n",
      "\t0.08s\t = Validation runtime\n",
      "Fitting model: XGBoost ...\n",
      "\t0.996\t = Validation score   (accuracy)\n",
      "\t7.45s\t = Training   runtime\n",
      "\t0.01s\t = Validation runtime\n",
      "Fitting model: NeuralNetTorch ...\n",
      "\t0.988\t = Validation score   (accuracy)\n",
      "\t32.65s\t = Training   runtime\n",
      "\t0.01s\t = Validation runtime\n",
      "Fitting model: LightGBMLarge ...\n",
      "\t0.994\t = Validation score   (accuracy)\n",
      "\t17.56s\t = Training   runtime\n",
      "\t0.03s\t = Validation runtime\n",
      "Fitting model: WeightedEnsemble_L2 ...\n",
      "\tEnsemble Weights: {'XGBoost': 1.0}\n",
      "\t0.996\t = Validation score   (accuracy)\n",
      "\t0.16s\t = Training   runtime\n",
      "\t0.0s\t = Validation runtime\n",
      "AutoGluon training complete, total runtime = 232.22s ... Best model: WeightedEnsemble_L2 | Estimated inference throughput: 167705.1 rows/s (2500 batch size)\n",
      "TabularPredictor saved. To load, use: predictor = TabularPredictor.load(\"AutogluonModels/ag-20240824_143209\")\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "y_true contains only one label (12). Please provide the true labels explicitly through the labels argument.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[12], line 35\u001b[0m\n\u001b[1;32m     33\u001b[0m \u001b[38;5;66;03m# Calculate accuracy and log loss for the current fraud type\u001b[39;00m\n\u001b[1;32m     34\u001b[0m accuracy \u001b[38;5;241m=\u001b[39m accuracy_score(true_labels, predictions)\n\u001b[0;32m---> 35\u001b[0m logloss \u001b[38;5;241m=\u001b[39m \u001b[43mlog_loss\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrue_labels\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mprobabilities\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     37\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFraud Type: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfraud_type\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     38\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m - Accuracy: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00maccuracy\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/sklearn/utils/_param_validation.py:213\u001b[0m, in \u001b[0;36mvalidate_params.<locals>.decorator.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    207\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    208\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m config_context(\n\u001b[1;32m    209\u001b[0m         skip_parameter_validation\u001b[38;5;241m=\u001b[39m(\n\u001b[1;32m    210\u001b[0m             prefer_skip_nested_validation \u001b[38;5;129;01mor\u001b[39;00m global_skip_validation\n\u001b[1;32m    211\u001b[0m         )\n\u001b[1;32m    212\u001b[0m     ):\n\u001b[0;32m--> 213\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    214\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m InvalidParameterError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    215\u001b[0m     \u001b[38;5;66;03m# When the function is just a wrapper around an estimator, we allow\u001b[39;00m\n\u001b[1;32m    216\u001b[0m     \u001b[38;5;66;03m# the function to delegate validation to the estimator, but we replace\u001b[39;00m\n\u001b[1;32m    217\u001b[0m     \u001b[38;5;66;03m# the name of the estimator by the name of the function in the error\u001b[39;00m\n\u001b[1;32m    218\u001b[0m     \u001b[38;5;66;03m# message to avoid confusion.\u001b[39;00m\n\u001b[1;32m    219\u001b[0m     msg \u001b[38;5;241m=\u001b[39m re\u001b[38;5;241m.\u001b[39msub(\n\u001b[1;32m    220\u001b[0m         \u001b[38;5;124mr\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mparameter of \u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mw+ must be\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m    221\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mparameter of \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfunc\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__qualname__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m must be\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m    222\u001b[0m         \u001b[38;5;28mstr\u001b[39m(e),\n\u001b[1;32m    223\u001b[0m     )\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/sklearn/metrics/_classification.py:2917\u001b[0m, in \u001b[0;36mlog_loss\u001b[0;34m(y_true, y_pred, eps, normalize, sample_weight, labels)\u001b[0m\n\u001b[1;32m   2915\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(lb\u001b[38;5;241m.\u001b[39mclasses_) \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[1;32m   2916\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m labels \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m-> 2917\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m   2918\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124my_true contains only one label (\u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;124m). Please \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   2919\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mprovide the true labels explicitly through the \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   2920\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlabels argument.\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(lb\u001b[38;5;241m.\u001b[39mclasses_[\u001b[38;5;241m0\u001b[39m])\n\u001b[1;32m   2921\u001b[0m         )\n\u001b[1;32m   2922\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   2923\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m   2924\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mThe labels array needs to contain at least two \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   2925\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlabels for log_loss, \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   2926\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mgot \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(lb\u001b[38;5;241m.\u001b[39mclasses_)\n\u001b[1;32m   2927\u001b[0m         )\n",
      "\u001b[0;31mValueError\u001b[0m: y_true contains only one label (12). Please provide the true labels explicitly through the labels argument."
     ]
    }
   ],
   "source": [
    "from autogluon.tabular import TabularPredictor\n",
    "\n",
    "\n",
    "train_data = train_x_encoded.copy()\n",
    "train_data['Fraud_Type'] = train_y_encoded\n",
    "\n",
    "from sklearn.metrics import accuracy_score, log_loss\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "train_data_split, val_data_split = train_test_split(train_data, test_size=0.2, random_state=42, stratify=train_data['Fraud_Type'])\n",
    "\n",
    "# Train the model\n",
    "predictor = TabularPredictor(label='Fraud_Type', eval_metric='accuracy').fit(train_data_split)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fraud Type: 12\n",
      " - Accuracy: 0.9997576736672051\n",
      " - Log Loss: Cannot calculate log loss with only one class present.\n",
      "\n",
      "Fraud Type: 9\n",
      " - Accuracy: 0.9852941176470589\n",
      " - Log Loss: Cannot calculate log loss with only one class present.\n",
      "\n",
      "Fraud Type: 7\n",
      " - Accuracy: 0.9872549019607844\n",
      " - Log Loss: Cannot calculate log loss with only one class present.\n",
      "\n",
      "Fraud Type: 2\n",
      " - Accuracy: 0.9705882352941176\n",
      " - Log Loss: Cannot calculate log loss with only one class present.\n",
      "\n",
      "Fraud Type: 11\n",
      " - Accuracy: 0.9745098039215686\n",
      " - Log Loss: Cannot calculate log loss with only one class present.\n",
      "\n",
      "Fraud Type: 10\n",
      " - Accuracy: 0.9911764705882353\n",
      " - Log Loss: Cannot calculate log loss with only one class present.\n",
      "\n",
      "Fraud Type: 8\n",
      " - Accuracy: 0.9774509803921568\n",
      " - Log Loss: Cannot calculate log loss with only one class present.\n",
      "\n",
      "Fraud Type: 4\n",
      " - Accuracy: 0.9901960784313726\n",
      " - Log Loss: Cannot calculate log loss with only one class present.\n",
      "\n",
      "Fraud Type: 5\n",
      " - Accuracy: 0.9911764705882353\n",
      " - Log Loss: Cannot calculate log loss with only one class present.\n",
      "\n",
      "Fraud Type: 0\n",
      " - Accuracy: 0.9941176470588236\n",
      " - Log Loss: Cannot calculate log loss with only one class present.\n",
      "\n",
      "Fraud Type: 6\n",
      " - Accuracy: 0.9862745098039216\n",
      " - Log Loss: Cannot calculate log loss with only one class present.\n",
      "\n",
      "Fraud Type: 1\n",
      " - Accuracy: 0.9980392156862745\n",
      " - Log Loss: Cannot calculate log loss with only one class present.\n",
      "\n",
      "Fraud Type: 3\n",
      " - Accuracy: 0.9754901960784313\n",
      " - Log Loss: Cannot calculate log loss with only one class present.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Make predictions on the validation set\n",
    "val_predictions = predictor.predict(val_data_split)\n",
    "val_probabilities = predictor.predict_proba(val_data_split)\n",
    "\n",
    "# Get true labels\n",
    "val_true_labels = val_data_split['Fraud_Type']\n",
    "\n",
    "# Calculate and print loss (accuracy and log loss) by fraud type\n",
    "fraud_types = val_true_labels.unique()\n",
    "for fraud_type in fraud_types:\n",
    "    # Get the indices for the current fraud type\n",
    "    indices = val_true_labels == fraud_type\n",
    "\n",
    "    # True labels and predictions for the current fraud type\n",
    "    true_labels = val_true_labels[indices]\n",
    "    predictions = val_predictions[indices]\n",
    "    probabilities = val_probabilities.loc[indices]\n",
    "\n",
    "    # Calculate accuracy\n",
    "    accuracy = accuracy_score(true_labels, predictions)\n",
    "    \n",
    "    # Handle single-class case for log loss\n",
    "    if len(true_labels.unique()) == 1:\n",
    "        print(f\"Fraud Type: {fraud_type}\")\n",
    "        print(f\" - Accuracy: {accuracy}\")\n",
    "        print(f\" - Log Loss: Cannot calculate log loss with only one class present.\\n\")\n",
    "    else:\n",
    "        logloss = log_loss(true_labels, probabilities)\n",
    "        print(f\"Fraud Type: {fraud_type}\")\n",
    "        print(f\" - Accuracy: {accuracy}\")\n",
    "        print(f\" - Log Loss: {logloss}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 테스트 데이터 인코딩\n",
    "test_x_encoded = test_x.copy()\n",
    "test_x_encoded[categorical_columns] = ordinal_encoder.transform(test_x[categorical_columns])\n",
    "\n",
    "\n",
    "# 특성 순서 맞추기 및 데이터 타입 일치\n",
    "test_x_encoded = test_x_encoded[feature_order]\n",
    "for col in feature_order:\n",
    "    test_x_encoded[col] = test_x_encoded[col].astype(train_x_encoded[col].dtype)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predict on test data\n",
    "predictions = predictor.predict(test_x_encoded)\n",
    "\n",
    "# Reverse transform to get original labels if necessary\n",
    "predictions_label = le_subclass.inverse_transform(predictions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Submission"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ID</th>\n",
       "      <th>Fraud_Type</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>TEST_000000</td>\n",
       "      <td>b</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>TEST_000001</td>\n",
       "      <td>m</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>TEST_000002</td>\n",
       "      <td>m</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>TEST_000003</td>\n",
       "      <td>m</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>TEST_000004</td>\n",
       "      <td>h</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            ID Fraud_Type\n",
       "0  TEST_000000          b\n",
       "1  TEST_000001          m\n",
       "2  TEST_000002          m\n",
       "3  TEST_000003          m\n",
       "4  TEST_000004          h"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 분류 예측 결과 제출 데이터프레임(DataFrame)\n",
    "# 분류 예측 결과 데이터프레임 파일명을 반드시 clf_submission.csv 로 지정해야합니다.\n",
    "clf_submission = pd.read_csv(\"/workspace/Dataset/FSI/sample_submission.csv\")\n",
    "clf_submission[\"Fraud_Type\"] = predictions_label\n",
    "clf_submission.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Customer_Birthyear</th>\n",
       "      <th>Customer_Gender</th>\n",
       "      <th>Customer_personal_identifier</th>\n",
       "      <th>Customer_identification_number</th>\n",
       "      <th>Customer_registration_datetime</th>\n",
       "      <th>Customer_credit_rating</th>\n",
       "      <th>Customer_flag_change_of_authentication_1</th>\n",
       "      <th>Customer_flag_change_of_authentication_2</th>\n",
       "      <th>Customer_flag_change_of_authentication_3</th>\n",
       "      <th>Customer_flag_change_of_authentication_4</th>\n",
       "      <th>...</th>\n",
       "      <th>Last_atm_transaction_datetime</th>\n",
       "      <th>Last_bank_branch_transaction_datetime</th>\n",
       "      <th>Flag_deposit_more_than_tenMillion</th>\n",
       "      <th>Unused_account_status</th>\n",
       "      <th>Recipient_account_suspend_status</th>\n",
       "      <th>Number_of_transaction_with_the_account</th>\n",
       "      <th>Transaction_history_with_the_account</th>\n",
       "      <th>First_time_iOS_by_vulnerable_user</th>\n",
       "      <th>Fraud_Type</th>\n",
       "      <th>Transaction_resumed_date</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1977</td>\n",
       "      <td>male</td>\n",
       "      <td>장영식</td>\n",
       "      <td>DuWOqP-DWQrklO</td>\n",
       "      <td>2012-12-10 22:02:43</td>\n",
       "      <td>B</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>2004-07-22 11:07:29</td>\n",
       "      <td>2012-04-30 20:27:48</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>m</td>\n",
       "      <td>2036-04-29 15:53:01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1973</td>\n",
       "      <td>male</td>\n",
       "      <td>강지우</td>\n",
       "      <td>FZOPOt-CmkFKxG</td>\n",
       "      <td>2010-10-10 18:02:32</td>\n",
       "      <td>A</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>2013-07-09 21:00:28</td>\n",
       "      <td>2019-02-07 02:33:16</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>m</td>\n",
       "      <td>2011-12-18 17:32:43</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1979</td>\n",
       "      <td>male</td>\n",
       "      <td>우지혜</td>\n",
       "      <td>LJfpJX-lNognsH</td>\n",
       "      <td>2012-12-10 22:02:43</td>\n",
       "      <td>B</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>2031-03-12 22:37:46</td>\n",
       "      <td>2011-09-10 13:02:51</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>m</td>\n",
       "      <td>2028-10-30 02:16:31</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2002</td>\n",
       "      <td>female</td>\n",
       "      <td>이윤서</td>\n",
       "      <td>KpdklD-ymHOSLQ</td>\n",
       "      <td>2007-06-08 19:44:42</td>\n",
       "      <td>B</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>2012-06-12 05:35:25</td>\n",
       "      <td>2025-06-14 07:48:55</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>m</td>\n",
       "      <td>2019-11-26 03:28:21</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1992</td>\n",
       "      <td>female</td>\n",
       "      <td>우서현</td>\n",
       "      <td>BDBAtF-ZmBUHYl</td>\n",
       "      <td>2007-10-28 22:46:17</td>\n",
       "      <td>C</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>2009-03-24 13:53:00</td>\n",
       "      <td>2018-03-10 01:14:36</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>m</td>\n",
       "      <td>2025-06-30 21:01:03</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 63 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   Customer_Birthyear Customer_Gender Customer_personal_identifier  \\\n",
       "0                1977            male                          장영식   \n",
       "1                1973            male                          강지우   \n",
       "2                1979            male                          우지혜   \n",
       "3                2002          female                          이윤서   \n",
       "4                1992          female                          우서현   \n",
       "\n",
       "  Customer_identification_number Customer_registration_datetime  \\\n",
       "0                 DuWOqP-DWQrklO            2012-12-10 22:02:43   \n",
       "1                 FZOPOt-CmkFKxG            2010-10-10 18:02:32   \n",
       "2                 LJfpJX-lNognsH            2012-12-10 22:02:43   \n",
       "3                 KpdklD-ymHOSLQ            2007-06-08 19:44:42   \n",
       "4                 BDBAtF-ZmBUHYl            2007-10-28 22:46:17   \n",
       "\n",
       "  Customer_credit_rating  Customer_flag_change_of_authentication_1  \\\n",
       "0                      B                                         1   \n",
       "1                      A                                         1   \n",
       "2                      B                                         1   \n",
       "3                      B                                         1   \n",
       "4                      C                                         1   \n",
       "\n",
       "   Customer_flag_change_of_authentication_2  \\\n",
       "0                                         1   \n",
       "1                                         1   \n",
       "2                                         1   \n",
       "3                                         1   \n",
       "4                                         1   \n",
       "\n",
       "   Customer_flag_change_of_authentication_3  \\\n",
       "0                                         1   \n",
       "1                                         1   \n",
       "2                                         0   \n",
       "3                                         1   \n",
       "4                                         1   \n",
       "\n",
       "   Customer_flag_change_of_authentication_4  ...  \\\n",
       "0                                         1  ...   \n",
       "1                                         1  ...   \n",
       "2                                         1  ...   \n",
       "3                                         1  ...   \n",
       "4                                         1  ...   \n",
       "\n",
       "   Last_atm_transaction_datetime  Last_bank_branch_transaction_datetime  \\\n",
       "0            2004-07-22 11:07:29                    2012-04-30 20:27:48   \n",
       "1            2013-07-09 21:00:28                    2019-02-07 02:33:16   \n",
       "2            2031-03-12 22:37:46                    2011-09-10 13:02:51   \n",
       "3            2012-06-12 05:35:25                    2025-06-14 07:48:55   \n",
       "4            2009-03-24 13:53:00                    2018-03-10 01:14:36   \n",
       "\n",
       "   Flag_deposit_more_than_tenMillion Unused_account_status  \\\n",
       "0                                  0                     0   \n",
       "1                                  1                     1   \n",
       "2                                  0                     1   \n",
       "3                                  0                     1   \n",
       "4                                  0                     1   \n",
       "\n",
       "   Recipient_account_suspend_status  Number_of_transaction_with_the_account  \\\n",
       "0                                 1                                       0   \n",
       "1                                 1                                       0   \n",
       "2                                 0                                       0   \n",
       "3                                 0                                       0   \n",
       "4                                 0                                       2   \n",
       "\n",
       "   Transaction_history_with_the_account  First_time_iOS_by_vulnerable_user  \\\n",
       "0                                     0                                  0   \n",
       "1                                     2                                  0   \n",
       "2                                     0                                  0   \n",
       "3                                     2                                  0   \n",
       "4                                     0                                  0   \n",
       "\n",
       "   Fraud_Type  Transaction_resumed_date  \n",
       "0           m       2036-04-29 15:53:01  \n",
       "1           m       2011-12-18 17:32:43  \n",
       "2           m       2028-10-30 02:16:31  \n",
       "3           m       2019-11-26 03:28:21  \n",
       "4           m       2025-06-30 21:01:03  \n",
       "\n",
       "[5 rows x 63 columns]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 합성 데이터 생성 결과 제출 데이터프레임(DataFrame)\n",
    "# 합성 데이터 생성 결과 데이터프레임 파일명을 반드시 syn_submission.csv 로 지정해야합니다.\n",
    "all_synthetic_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done.\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "(*) 저장 시 각 파일명을 반드시 확인해주세요.\n",
    "    1. 분류 예측 결과 데이터프레임 파일명 = clf_submission.csv\n",
    "    2. 합성 데이터 생성 결과 데이터프레임 파일명 = syn_submission.csv\n",
    "\n",
    "(*) 제출 파일(zip) 내에 두 개의 데이터프레임이 각각 위의 파일명으로 반드시 존재해야합니다.\n",
    "(*) 파일명을 일치시키지 않으면 채점이 불가능합니다.\n",
    "'''\n",
    "\n",
    "# 폴더 생성 및 작업 디렉토리 변경\n",
    "os.makedirs('/workspace/Dataset/FSI/baseline+autogluon+datainsert/submission', exist_ok=True)\n",
    "os.chdir(\"/workspace/Dataset/FSI/baseline+autogluon+datainsert/submission/\")\n",
    "\n",
    "# CSV 파일로 저장\n",
    "clf_submission.to_csv('/workspace/Dataset/FSI/baseline+autogluon+datainsert/submission/clf_submission.csv', encoding='UTF-8-sig', index=False)\n",
    "all_synthetic_data.to_csv('/workspace/Dataset/FSI/baseline+autogluon+datainsert/submission/syn_submission.csv', encoding='UTF-8-sig', index=False)\n",
    "\n",
    "# ZIP 파일 생성 및 CSV 파일 추가\n",
    "with zipfile.ZipFile(\"/workspace/Dataset/FSI/baseline+autogluon+datainsert/submission/baseline_submission.zip\", 'w') as submission:\n",
    "    submission.write('clf_submission.csv')\n",
    "    submission.write('syn_submission.csv')\n",
    "    \n",
    "print('Done.')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
